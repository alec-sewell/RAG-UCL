import pandas as pd
from sklearn.metrics import cohen_kappa_score
import io


data = """
Question N,Answer_ID,AI Model,H1_Correct,H1_Relevant,H1_Complete,H1_Hallucination,H2_Correct,H2_Relevant,H2_Hallucination,Mean Correct,Mean Relevant,Mean Complete
1,1A,NBLM,2,2,2,0,3,3,0,2.5,2.5,2.5
1,1B,RALF,1,1,1,0,3,3,0,2.0,2.0,1.5
1,1C,CGPT,1,1,1,0,1,1,0,1.0,1.0,1.5
2,2A,RALF,1,1,1,1,1,1,0,1.0,1.0,1.0
2,2B,CGPT,3,3,3,0,3,3,1,3.0,3.0,3.0
2,2C,NBLM,3,3,3,0,2,2,0,2.5,2.5,2.5
3,3A,CGPT,1,2,1,0,2,3,0,1.5,2.5,1.5
3,3B,RALF,3,3,3,0,4,4,1,3.5,3.5,3.5
3,3C,NBLM,3,3,3,0,4,4,1,3.5,3.5,3.5
4,4A,RALF,5,5,4,1,5,5,0,5.0,5.0,4.5
4,4B,NBLM,5,5,5,1,3,3,0,4.0,4.0,4.0
4,4C,CGPT,3,3,3,0,2,2,0,2.5,2.5,2.5
5,5A,CGPT,3,3,3,0,4,4,0,3.5,3.5,3.5
5,5B,NBLM,1,1,1,0,2,2,0,1.5,1.5,1.5
5,5C,RALF,1,1,1,1,1,1,0,1.0,1.0,1.0
6,6A,NBLM,3,3,3,0,5,5,1,4.0,4.0,4.0
6,6B,CGPT,2,2,2,0,5,5,1,3.5,3.5,2.5
6,6C,RALF,1,1,1,0,3,3,0,2.0,2.0,1.5
7,7A,RALF,1,1,1,0,1,1,0,1.0,1.0,1.0
7,7B,CGPT,5,5,5,1,5,5,1,5.0,5.0,5.0
7,7C,NBLM,5,5,5,1,5,5,1,5.0,5.0,5.0
8,8A,CGPT,4,4,4,0,4,3,0,4.0,3.5,3.5
8,8B,RALF,4,4,4,0,4,3,0,4.0,3.5,3.5
8,8C,NBLM,3,3,3,0,3,2,0,3.0,2.5,2.5
9,9A,NBLM,3,3,3,0,2,3,0,2.5,3.0,2.5
9,9B,RALF,5,5,3,1,5,5,1,5.0,5.0,4.0
9,9C,CGPT,4,4,4,0,5,5,1,4.5,4.5,4.5
"""

df = pd.read_csv(io.StringIO(data))

# For this calculation, we only need the raw scores from each human.
h1_correct = df['H1_Correct']
h2_correct = df['H2_Correct']

h1_relevant = df['H1_Relevant']
h2_relevant = df['H2_Relevant']

h1_hallucination = df['H1_Hallucination']
h2_hallucination = df['H2_Hallucination']

# 2. Calculate Cohen's Kappa for each metric
kappa_correct = cohen_kappa_score(h1_correct, h2_correct)
kappa_relevant = cohen_kappa_score(h1_relevant, h2_relevant)
kappa_hallucination = cohen_kappa_score(h1_hallucination, h2_hallucination)

# 3. a "weighted" Kappa.
# This gives partial credit for close disagreements (e.g., a 4 vs 5 is better than a 1 vs 5).
weighted_kappa_correct = cohen_kappa_score(h1_correct, h2_correct, weights='linear')
weighted_kappa_relevant = cohen_kappa_score(h1_relevant, h2_relevant, weights='linear')



print("Inter-Rater Reliability Analysis:")
print("-" * 35)
print(f"Correctness (Standard Kappa): {kappa_correct:.2f} (Fair Agreement)")
print(f"Relevance (Standard Kappa):   {kappa_relevant:.2f} (Fair Agreement)")
print(f"Hallucination (Standard Kappa): {kappa_hallucination:.2f} (Moderate Agreement)")
print("-" * 35)
print("Analysis with Weighted Kappa (for ordinal scales):")
print(f"Correctness (Weighted Kappa): {weighted_kappa_correct:.2f} (X Agreement)")
print(f"Relevance (Weighted Kappa):   {weighted_kappa_relevant:.2f} (X Agreement)")
