import pandas as pd
from sklearn.metrics import cohen_kappa_score
import io


data = """
Question N,Answer_ID,AI Model,H1_Correct,H1_Relevant,H1_Complete,H1_Hallucination,H2_Correct,H2_Relevant,H2_Hallucination,Mean Correct,Mean Relevant,Mean Complete
1,1A,NBLM,2,2,2,0,3,3,0,2.5,2.5,2.5
1,1B,RALF,1,1,1,0,3,3,0,2.0,2.0,1.5
1,1C,CGPT,1,1,1,0,1,1,0,1.0,1.0,1.5
2,2A,RALF,1,1,1,1,1,1,0,1.0,1.0,1.0
2,2B,CGPT,3,3,3,0,3,3,1,3.0,3.0,3.0
2,2C,NBLM,3,3,3,0,2,2,0,2.5,2.5,2.5
3,3A,CGPT,1,2,1,0,2,3,0,1.5,2.5,1.5
3,3B,RALF,3,3,3,0,4,4,1,3.5,3.5,3.5
3,3C,NBLM,3,3,3,0,4,4,1,3.5,3.5,3.5
4,4A,RALF,5,5,4,1,5,5,0,5.0,5.0,4.5
4,4B,NBLM,5,5,5,1,3,3,0,4.0,4.0,4.0
4,4C,CGPT,3,3,3,0,2,2,0,2.5,2.5,2.5
5,5A,CGPT,3,3,3,0,4,4,0,3.5,3.5,3.5
5,5B,NBLM,1,1,1,0,2,2,0,1.5,1.5,1.5
5,5C,RALF,1,1,1,1,1,1,0,1.0,1.0,1.0
6,6A,NBLM,3,3,3,0,5,5,1,4.0,4.0,4.0
6,6B,CGPT,2,2,2,0,5,5,1,3.5,3.5,2.5
6,6C,RALF,1,1,1,0,3,3,0,2.0,2.0,1.5
7,7A,RALF,1,1,1,0,1,1,0,1.0,1.0,1.0
7,7B,CGPT,5,5,5,1,5,5,1,5.0,5.0,5.0
7,7C,NBLM,5,5,5,1,5,5,1,5.0,5.0,5.0
8,8A,CGPT,4,4,4,0,4,3,0,4.0,3.5,3.5
8,8B,RALF,4,4,4,0,4,3,0,4.0,3.5,3.5
8,8C,NBLM,3,3,3,0,3,2,0,3.0,2.5,2.5
9,9A,NBLM,3,3,3,0,2,3,0,2.5,3.0,2.5
9,9B,RALF,5,5,3,1,5,5,1,5.0,5.0,4.0
9,9C,CGPT,4,4,4,0,5,5,1,4.5,4.5,4.5
"""

df = pd.read_csv(io.StringIO(data))

metrics_config = {
    "Correctness": {"col": "Correct", "agreement": "Fair Agreement"},
    "Relevance": {"col": "Relevant", "agreement": "Fair Agreement"},
    "Hallucination": {"col": "Hallucination", "agreement": "Moderate Agreement"}
}

print("Inter-Rater Reliability Analysis:")
print("-" * 35)

for name, config in metrics_config.items():
    kappa = cohen_kappa_score(df[f'H1_{config["col"]}'], df[f'H2_{config["col"]}'])
    label = f"{name} (Standard Kappa):"

    if name == "Relevance":
        print(f"{label}   {kappa:.2f} ({config['agreement']})")
    else:
        print(f"{label} {kappa:.2f} ({config['agreement']})")

print("-" * 35)
print("Analysis with Weighted Kappa (for ordinal scales):")

# Calculate and print weighted Kappa scores
for name, config in metrics_config.items():
    if name == "Hallucination":
        continue

    weighted_kappa = cohen_kappa_score(df[f'H1_{config["col"]}'], df[f'H2_{config["col"]}'], weights='linear')
    label = f"{name} (Weighted Kappa):"

    # Preserve original output's spacing
    if name == "Relevance":
        print(f"{label}   {weighted_kappa:.2f} (X Agreement)")
    else:
        print(f"{label} {weighted_kappa:.2f} (X Agreement)")