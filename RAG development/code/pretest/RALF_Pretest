import io
import os

import filetype
import gradio as gr
import pandas as pd
import pymupdf as fitz
from dotenv import load_dotenv
from langchain.chains import LLMChain, RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from PIL import Image

# Load environment variables
load_dotenv()

# Constants
CHUNK_SIZE = 600  # Adjusted for better balance with multiple docs
CHUNK_OVERLAP = 100
MAX_TOKENS = 4096
MODEL_NAME = "gpt-4o"
TEMPERATURE = 0.7

# Get OpenAI API key from environment variable
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if not OPENAI_API_KEY:
    raise ValueError("Please set OPENAI_API_KEY in your .env file")

# Initialize LLM
llm = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    max_tokens=MAX_TOKENS
)

PROMPT = PromptTemplate(
    template="""Context: {context}
Question: {question}

You are a search bot that forms a coherent answer to a user query based on search results that are provided to you.
If the search results are irrelevant to the question, respond with "I do not have enough information to answer this question."
Do not base your response on information or knowledge that is not in the search results.
Make sure your response is answering the query asked
Consider that each search result is a partial segment from a bigger text, and may be incomplete.

If the question is about images or tables, refer to them specifically in your answer.""",
    input_variables=["context", "question"]
)


def process_single_pdf_text(pdf_path: str) -> list[str]:
    """
    Extracts text from a single PDF and splits it into chunks.

    Args:
        pdf_path: The file path to the PDF document.

    Returns:
        A list of text chunks.
    """
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        # Use get_text() for cleaner extraction. 'text' is the default output format.
        text += page.get_text()
    doc.close()

    # Use RecursiveCharacterTextSplitter for robust splitting
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    texts = text_splitter.split_text(text)
    return texts


def extract_images_and_tables_single_pdf(pdf_path: str) -> tuple[list[tuple], list[tuple]]:
    """
    Extracts images and tables from a single PDF.

    Args:
        pdf_path: The file path to the PDF document.

    Returns:
        A tuple containing a list of extracted images and a list of extracted tables.
        Each image is a tuple of (metadata_string, PIL.Image).
        Each table is a tuple of (metadata_string, pd.DataFrame).
    """
    doc = fitz.open(pdf_path)
    images = []
    tables = []

    for page_num, page in enumerate(doc):
        image_list = page.get_images(full=True)

        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image = Image.open(io.BytesIO(image_bytes))
            # Include PDF name in metadata for clarity in multi-doc scenario
            images.append(
                (f"PDF: {os.path.basename(pdf_path)}, Page {page_num + 1}, Image {img_index + 1}", image)
            )

        tables_on_page = page.find_tables()
        for table_index, table in enumerate(tables_on_page):
            df = pd.DataFrame(table.extract())
            # Include PDF name in metadata for clarity in multi-doc scenario
            tables.append(
                (f"PDF: {os.path.basename(pdf_path)}, Page {page_num + 1}, Table {table_index + 1}", df)
            )

    doc.close()
    return images, tables


def create_retrievers(all_texts: list[str]) -> tuple:
    """
    Creates dense and sparse retrievers from text chunks.

    Args:
        all_texts: A list of text chunks.

    Returns:
        A tuple containing the dense retriever (FAISS) and the sparse retriever (BM25).
    """
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_texts(all_texts, embeddings)
    dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 8})

    sparse_retriever = BM25Retriever.from_texts(all_texts)
    sparse_retriever.k = 4

    return dense_retriever, sparse_retriever


def expand_query(query: str, llm: ChatOpenAI) -> str:
    """
    Expands the original query with related terms using an LLM.

    The function uses the 'invoke' method with a dictionary, which is the
    current standard for the LangChain library. The response from the LLM is a
    dictionary, and the expanded terms are in the 'text' key.

    Args:
        query: The original user query.
        llm: The ChatOpenAI model instance.

    Returns:
        The expanded query string.
    """
    prompt = PromptTemplate(
        input_variables=["query"],
        template="""Given the following query, generate 3-5 related terms or phrases that could be relevant to the query. 
        Separate the terms with commas.

        Query: {query}

        Related terms:"""
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    response = chain.invoke({"query": query})
    expanded_terms = [term.strip() for term in response['text'].split(',')]
    expanded_query = f"{query} {' '.join(expanded_terms)}"
    return expanded_query


def rag_pipeline(query: str, qa_chain, retriever, images: list, tables: list, llm_instance: ChatOpenAI) -> tuple[str, str]:
    """
    Executes the RAG pipeline: expands query, retrieves documents, and generates an answer.

    Note: This pipeline expands the query for logging retrieved chunks, but the final
    answer is generated using documents retrieved with the original, un-expanded query.

    Args:
        query: The user's query.
        qa_chain: The RetrievalQA chain.
        retriever: The document retriever.
        images: A list of extracted images.
        tables: A list of extracted tables.
        llm_instance: The language model instance for query expansion.

    Returns:
        A tuple containing the generated answer and a log of the process.
    """
    expanded_query = expand_query(query, llm_instance)
    # Documents are retrieved here with the expanded query for logging purposes.
    relevant_docs_for_log = retriever.get_relevant_documents(expanded_query)

    log = "Query Expansion:\n"
    log += f"Original query: {query}\n"
    log += f"Expanded query: {expanded_query}\n\n"
    log += "Relevant chunks (Hybrid):\n"

    # EnsembleRetriever doesn't provide a score, so it's not logged.
    for i, doc in enumerate(relevant_docs_for_log, 1):
        log += f"Chunk {i}:\n"
        log += f"Sample: {doc.page_content[:200]}...\n\n"

    log += f"Number of images in all PDFs: {len(images)}\n"
    log += f"Number of tables in all PDFs: {len(tables)}\n\n"

    # The qa_chain is invoked with the original query. It will internally re-run the
    # retriever with this original query.
    response = qa_chain.invoke({"query": query})
    return response['result'], log


def process_pdfs_and_query(pdf_files_list: list, query: str) -> tuple[str, str]:
    """
    Processes multiple PDF files, extracts content, builds a RAG pipeline, and answers a query.

    Args:
        pdf_files_list: A list of uploaded PDF file objects from Gradio.
        query: The user's query.

    Returns:
        A tuple containing the answer to the query and a detailed processing log.
    """
    all_texts = []
    all_images = []
    all_tables = []
    processing_log = "--- PDF Processing Log ---\n"

    if not pdf_files_list:
        return "No PDF files uploaded.", processing_log

    for pdf_file_obj in pdf_files_list:
        pdf_path = pdf_file_obj.name  # Gradio File object has a 'name' attribute for the temp path

        # Validate file type to ensure it's a PDF
        actual_mime_type = "UNKNOWN"
        try:
            actual_mime_type = filetype.guess_mime(pdf_path)
        except Exception as e:
            processing_log += f"  - WARNING: Could not determine MIME type for {os.path.basename(pdf_path)}: {e}\n"

        if actual_mime_type != "application/pdf":
            processing_log += f"  - SKIPPING: '{os.path.basename(pdf_path)}' is not a PDF (detected as {actual_mime_type}).\n"
            continue

        processing_log += f"Processing PDF: {os.path.basename(pdf_path)}\n"
        try:
            texts_from_pdf = process_single_pdf_text(pdf_path)
            images_from_pdf, tables_from_pdf = extract_images_and_tables_single_pdf(pdf_path)

            all_texts.extend(texts_from_pdf)
            all_images.extend(images_from_pdf)
            all_tables.extend(tables_from_pdf)

            processing_log += f"  - Chunks extracted: {len(texts_from_pdf)}\n"
            processing_log += f"  - Images extracted: {len(images_from_pdf)}\n"
            processing_log += f"  - Tables extracted: {len(tables_from_pdf)}\n"
        except Exception as e:
            processing_log += f"  - ERROR processing {os.path.basename(pdf_path)}: {e}\n"
            continue

    if not all_texts:
        return "No text content could be extracted from any of the provided PDFs. Please ensure they contain selectable text.", processing_log

    processing_log += "\n--- Combined Processing Summary ---\n"
    processing_log += f"Total number of text chunks from all PDFs: {len(all_texts)}\n"
    processing_log += f"Total number of images extracted from all PDFs: {len(all_images)}\n"
    processing_log += f"Total number of tables extracted from all PDFs: {len(all_tables)}\n\n"

    dense_retriever, sparse_retriever = create_retrievers(all_texts)

    hybrid_retriever = EnsembleRetriever(
        retrievers=[dense_retriever, sparse_retriever],
        weights=[0.5, 0.5],  # These weights balance dense vs. sparse search
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=hybrid_retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )

    print("\nRAG Pipeline initialized. Running query...")
    result, chunks_log = rag_pipeline(query, qa_chain, hybrid_retriever, all_images, all_tables, llm)

    final_log = processing_log + chunks_log

    return result, final_log


def gradio_interface(pdf_files_list_from_gradio: list, query: str) -> tuple[str, str]:
    """
    Gradio interface function that handles file uploads and queries.

    Gradio passes a list of file objects if file_count="multiple". This function
    passes that list to the main processing function.

    Args:
        pdf_files_list_from_gradio: A list of file objects from the Gradio file input.
        query: The user's query from the Gradio textbox.

    Returns:
        A tuple containing the answer and the processing log to be displayed in the UI.
    """
    if not pdf_files_list_from_gradio:
        return "Please upload at least one PDF file.", "No PDFs uploaded."

    result, full_log = process_pdfs_and_query(pdf_files_list_from_gradio, query)

    return result, full_log


def main():
    """Main function to launch the Gradio interface."""
    iface = gr.Interface(
        fn=gradio_interface,
        inputs=[
            gr.File(label="Upload PDFs", file_count="multiple"),
            gr.Textbox(label="Enter your question")
        ],
        outputs=[
            gr.Textbox(label="Answer"),
            gr.Textbox(label="Processing Log")
        ],
        title="RAG Pretest (Multiple PDFs)",
        description="Langchain+Chatgpt4omini Pretest UI for WELL Certification with multi-PDF support."
    )

    iface.launch()


if __name__ == "__main__":
    main()