import os
from dotenv import load_dotenv
import json
from datetime import datetime, timezone
import uuid
import pathlib
import gradio as gr
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI 
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever 
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.retrievers import ContextualCompressionRetriever
import hashlib, sqlite3, pickle, time, random
from typing import List, Optional
from langchain.embeddings.base import Embeddings
from langchain_google_genai._common import GoogleGenerativeAIError
from langchain.schema import Document
import numpy as np
import time
import random
import csv
import tempfile
# from langchain_community.document_loaders import BSHTMLLoader # This was for web scraping HTML, not needed for local PDF
from langchain.memory import ConversationBufferMemory

import pymupdf as fitz
import pytesseract # Make sure pytesseract is installed and Tesseract OCR is configured if you're using it for images
from PIL import Image
import io
import re
import pandas as pd
import filetype # <--- NEW: Import filetype

from typing import List, Tuple, Dict, Any

# ---- In-memory session cache (lives while this Python process runs) ----
INDEX_CACHE: Dict[str, Any] = {
    "signatures": None,        # list of (path, size, mtime) for current PDFs
    "hybrid_retriever": None,  # EnsembleRetriever
    "dense_retriever": None,   # FAISS retriever
    "sparse_retriever": None,  # BM25 retriever
    "images": None,            # to keep your log consistent
    "tables": None,
    "texts_count": 0,
}

def file_signatures(pdf_file_objs) -> List[Tuple[str, int, float]]:
    """Return a cheap signature for uploaded files: (path, size, mtime)."""
    sigs = []
    for f in (pdf_file_objs or []):
        p = f.name
        try:
            st = os.stat(p)
            sigs.append((p, st.st_size, st.st_mtime))
        except Exception:
            sigs.append((p, 0, 0.0))
    sigs.sort(key=lambda x: x[0])  # stable order
    return sigs

def signatures_changed(a, b) -> bool:
    return a != b


#------------------SETTING UP-----------------

# Load environment variables
load_dotenv()


BASE_DIR = pathlib.Path(__file__).resolve().parent
LOG_DIR = BASE_DIR / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

SESSION_ID = uuid.uuid4().hex[:8]
LOG_PATH = LOG_DIR / f"session-{SESSION_ID}.csv"

print(f"[LOG] Session log: {LOG_PATH}")

# Constants
CHUNK_SIZE = 1200 # Adjusted for better balance with multiple docs
CHUNK_OVERLAP = 150
MAX_TOKENS = 4096
MODEL_NAME = "gpt-4o"
TEMPERATURE = 0.7

# Get OpenAI API key from environment variable
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
if not GOOGLE_API_KEY:
    raise ValueError("Please set GOOGLE_API_KEY in your .env file")


#------------------Helpers Sleep/Metadata-----------------

# Small backoff helper 
def _sleep_backoff(attempt, base=1.5, jitter=0.25, cap=30):
    delay = min(cap, (base ** attempt)) + random.uniform(0, jitter)
    time.sleep(delay)

# RateLimited wrapper around any Embeddings 
class RateLimitedEmbeddings(Embeddings):
    def __init__(self, inner: Embeddings, batch_size: int = 64, max_retries: int = 6):
        self.inner = inner
        self.batch_size = batch_size
        self.max_retries = max_retries

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        out: List[List[float]] = []
        for start in range(0, len(texts), self.batch_size):
            batch = texts[start:start + self.batch_size]
            for attempt in range(self.max_retries):
                try:
                    out.extend(self.inner.embed_documents(batch))
                    break
                except GoogleGenerativeAIError as e:
                    # 429 / transient errors: retry with backoff
                    if "429" in str(e) or "exhausted" in str(e).lower():
                        _sleep_backoff(attempt)
                        continue
                    raise
        return out

    def embed_query(self, text: str) -> List[float]:
        for attempt in range(self.max_retries):
            try:
                return self.inner.embed_query(text)
            except GoogleGenerativeAIError as e:
                if "429" in str(e) or "exhausted" in str(e).lower():
                    _sleep_backoff(attempt)
                    continue
                raise
        # Fallback: last-resort return zeros of common dim
        return [0.0] * 768

# --- NEW: Simple on-disk cache (SQLite) for embeddings ---
class DiskCacheEmbeddings(Embeddings):
    def __init__(self, inner: Embeddings, db_path: pathlib.Path):
        self.inner = inner
        self.db_path = str(db_path)
        self._ensure_db()

    def _ensure_db(self):
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS embed_cache (
              key TEXT PRIMARY KEY,
              vec BLOB
            )
        """)
        con.commit()
        con.close()

    def _key(self, text: str) -> str:
        # include model class name to avoid collisions if you swap models
        h = hashlib.sha1(text.encode("utf-8")).hexdigest()
        return f"{self.inner.__class__.__name__}:{h}"

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()

        keys = [self._key(t) for t in texts]
        placeholders = ",".join("?" * len(keys))
        cached = {}
        if keys:
            cur.execute(f"SELECT key, vec FROM embed_cache WHERE key IN ({placeholders})", keys)
            for k, blob in cur.fetchall():
                cached[k] = pickle.loads(blob)

        missing_idx = [i for i, k in enumerate(keys) if k not in cached]
        missing_texts = [texts[i] for i in missing_idx]

        if missing_texts:
            new_vectors = self.inner.embed_documents(missing_texts)
            for i, vec in zip(missing_idx, new_vectors):
                k = keys[i]
                cur.execute(
                    "INSERT OR REPLACE INTO embed_cache (key, vec) VALUES (?, ?)",
                    (k, pickle.dumps(vec))
                )
            con.commit()

        # reconstruct in original order
        result = []
        for k in keys:
            if k in cached:
                result.append(cached[k])
            else:
                cur.execute("SELECT vec FROM embed_cache WHERE key = ?", (k,))
                row = cur.fetchone()
                result.append(pickle.loads(row[0]) if row else [0.0] * 768)
        con.close()
        return result

    def embed_query(self, text: str) -> List[float]:
        # query-path caching too (tiny)
        con = sqlite3.connect(self.db_path)
        cur = con.cursor()
        k = self._key("QUERY::" + text)
        cur.execute("SELECT vec FROM embed_cache WHERE key = ?", (k,))
        row = cur.fetchone()
        if row:
            con.close()
            return pickle.loads(row[0])
        vec = self.inner.embed_query(text)
        cur.execute("INSERT OR REPLACE INTO embed_cache (key, vec) VALUES (?, ?)", (k, pickle.dumps(vec)))
        con.commit()
        con.close()
        return vec


def read_pdf_pages(pdf_path: str):
    """Yield (page_number, text) for each page using PyMuPDF."""
    doc = fitz.open(pdf_path)
    try:
        for i, page in enumerate(doc, start=1):
            yield i, page.get_text()
    finally:
        doc.close()

def chunk_pdf_with_metadata(pdf_path: str, chunk_size: int, overlap: int):
    """
    Split each page to chunks; return a list[Document] with page + file metadata.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=overlap,
        separators=["\n\n", "\n", " ", ""]
    )
    docs = []
    file_name = os.path.basename(pdf_path)
    for page_num, page_text in read_pdf_pages(pdf_path):
        # create_documents preserves metadata for each produced chunk
        page_docs = splitter.create_documents(
            texts=[page_text],
            metadatas=[{
                "source_file": file_name,
                "page": page_num,
            }],
        )
        # add a stable chunk order within the page
        for idx, d in enumerate(page_docs):
            d.metadata["chunk"] = idx
        docs.extend(page_docs)
    return docs

def contextualize_documents(docs: list[Document], left: int = 1, right: int = 1):
    """
    For each center chunk, embed a window of neighbors.
    Returns a new list[Document] whose page_content = window(text),
    but whose metadata points to the *center* chunk.
    """
    # group by file to avoid crossing files
    by_file: dict[str, list[Document]] = {}
    for d in docs:
        by_file.setdefault(d.metadata.get("source_file", ""), []).append(d)

    ctx_docs: list[Document] = []
    for file_name, seq in by_file.items():
        # preserve original order: by (page, chunk)
        seq.sort(key=lambda d: (d.metadata.get("page", 0), d.metadata.get("chunk", 0)))
        n = len(seq)
        for i, center in enumerate(seq):
            s = max(0, i - left)
            e = min(n, i + right + 1)
            window_text = "\n\n".join(d.page_content for d in seq[s:e])

            # window page span & neighbors info
            window_pages = sorted({d.metadata.get("page") for d in seq[s:e]})
            md = dict(center.metadata)  # center metadata carried through
            md.update({
                "center_id": f'{file_name}::p{center.metadata.get("page")}::c{center.metadata.get("chunk")}',
                "window_left": left,
                "window_right": right,
                "window_pages": window_pages,
            })
            ctx_docs.append(Document(page_content=window_text, metadata=md))
    return ctx_docs



#-------------------Start of action------------------



# Initialize LLM
llm = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    model_name=MODEL_NAME,
    temperature=TEMPERATURE,
    max_tokens=MAX_TOKENS
)

# [ALEC] Changed the QA prompt
PROMPT = PromptTemplate(
    template="""Context: {context}


Question: {question}

You are a search bot that forms a coherent answer to a user query based on search results that are provided to you.
If the search results are irrelevant to the question, respond with "I do not have enough information to answer this question."
Do not base your response on information or knowledge that is not in the search results.
Make sure your response is answering the query asked
Consider that each search result is a partial segment from a bigger text, and may be incomplete.

If the question is about images or tables, refer to them specifically in your answer.""",
    input_variables=["context", "question"]
)

# --- One PDF processing at a time (called in a loop for multiple PDFs) ---
def process_pdfs_and_query(pdf_files_list, query):
    """Ensure index exists/reused, then answer the query."""
    processing_log = build_or_reuse_index(pdf_files_list)

    if INDEX_CACHE["hybrid_retriever"] is None:
        return "No index available. Please upload valid PDFs.", processing_log, []

    hybrid_retriever = INDEX_CACHE["hybrid_retriever"]
    all_images = INDEX_CACHE["images"] or []
    all_tables = INDEX_CACHE["tables"] or []

    
    print("\nRAG Pipeline initialized. Running query...")
    result, chunks_log, retrieved_chunks = rag_pipeline(query, hybrid_retriever, all_images, all_tables, llm)

    final_log = processing_log + chunks_log
    return result, final_log, retrieved_chunks


# --- Modified for single PDF processing (called in a loop for multiple PDFs) ---
def extract_images_and_tables_single_pdf(pdf_path): # Renamed function
    """Extract images and tables from a single PDF."""
    doc = fitz.open(pdf_path)
    images = []
    tables = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images(full=True)
        
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image = Image.open(io.BytesIO(image_bytes))
            # Include PDF name in metadata for clarity in multi-doc scenario
            images.append((f"PDF: {os.path.basename(pdf_path)}, Page {page_num + 1}, Image {img_index + 1}", image))
        
        tables_on_page = page.find_tables()
        for table_index, table in enumerate(tables_on_page):
            df = pd.DataFrame(table.extract())
            # Include PDF name in metadata for clarity in multi-doc scenario
            tables.append((f"PDF: {os.path.basename(pdf_path)}, Page {page_num + 1}, Table {table_index + 1}", df))
    
    return images, tables

# Define the path for your persistent cache database
EMBEDDING_CACHE_PATH = BASE_DIR / "embeddings_cache.db"

# Create the base embeddings model
base_embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001") 

# 1. Wrap it with your rate limiter and batcher
# The Google API limit is 100 docs per batch, so 64 is a safe number.
rate_limited_embeddings = RateLimitedEmbeddings(inner=base_embeddings, batch_size=64)

# 2. Wrap the result in your disk cache to avoid re-calculating embeddings
embeddings = DiskCacheEmbeddings(inner=rate_limited_embeddings, db_path=EMBEDDING_CACHE_PATH)

print(f"[CACHE] Using persistent disk cache for embeddings at: {EMBEDDING_CACHE_PATH}")


def create_retrievers(all_texts): 
    """Create embeddings and vector store from text chunks."""
    vectorstore = FAISS.from_texts(all_texts, embeddings)
    dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
    sparse_retriever = BM25Retriever.from_texts(all_texts)
    sparse_retriever.k = 4
    return dense_retriever, sparse_retriever

# Multi-query rewrite prompt (generates diverse alternatives)
MQR_PROMPT = PromptTemplate(
    input_variables=["question", "n"],
    template=(
        "You are a domain-savvy query rewriter. Generate {n} diverse reformulations of the question "
        "to capture synonyms, abbreviations, and likely headings found in building certification PDFs "
        "(e.g., WELL, Fitwel). Keep each on its own line, no numbering or extra text.\n\n"
        "Question: {question}\n\n"
        "Rewrites:"
    )
)
MQR_NUM = 4  




# --- Modified to accept llm_instance for expand_query ---
def rag_pipeline(query, retriever, images, tables, llm_instance):
    # Retrieve (MQR + Cross-encoder re-rank)
    relevant_docs = retriever.invoke(query)

    # Try to log generated alternates from MQR (best effort)
    generated_alts = []
    try:
        # If retriever is ContextualCompressionRetriever -> base_retriever is MQR
        base_r = getattr(retriever, "base_retriever", None)
        mqr_like = base_r if base_r is not None else retriever
        if hasattr(mqr_like, "llm_chain"):
            alt_text = mqr_like.llm_chain.invoke({"question": query})
            if isinstance(alt_text, dict) and "text" in alt_text:
                alt_text = alt_text["text"]
            if isinstance(alt_text, str):
                generated_alts = [line.strip() for line in alt_text.splitlines() if line.strip()]
    except Exception:
        pass  # non-fatal logging

    # Build log
    log = "Multi-Query Expansion + Cross-Encoder Re-rank:\n"
    log += f"Original query: {query}\n"
    if generated_alts:
        log += "Generated alternates:\n" + "\n".join(f"- {q}" for q in generated_alts) + "\n"
    log += "\nRelevant chunks (after re-rank):\n"

    retrieved_chunks = []
    context = ""
    for i, doc in enumerate(relevant_docs, 1):
        md = getattr(doc, "metadata", {}) or {}
        center_preview = md.get("center_preview", doc.page_content[:200])
        retrieved_chunks.append(center_preview)  # store the *center* view for CSV
        
        # feed the enriched text into context for the LLM:
        context += md.get("center_preview", "") + "\n\n"

        score = md.get("score") or md.get("relevance_score") or md.get("crossencoder_score")
        score_str = f" (score={score:.4f})" if isinstance(score, (int, float)) else ""
        log += (
            f"Chunk {i}{score_str} "
            f"[{md.get('source_file','?')} "
            f"center={md.get('center_index','?')} "
            f"win={md.get('window_left_index','?')}-{md.get('window_right_index','?')}] "
            f"center preview: {center_preview[:200]}...\n\n"
        )
    log += f"Number of images in all PDFs: {len(images)}\n"
    log += f"Number of tables in all PDFs: {len(tables)}\n\n"

    # 3) Create a combine chain with your prompt and pass the docs directly
    doc_chain = create_stuff_documents_chain(llm=llm_instance, prompt=PROMPT)
    raw = doc_chain.invoke({"context": relevant_docs, "question": query})
    answer = raw if isinstance(raw, str) else getattr(raw, "content", str(raw))
    return answer, log, retrieved_chunks



def safe_slug(text: str, max_len: int = 60) -> str:
    """Make a filesystem-safe short slug from text (for filenames)."""
    text = re.sub(r"\s+", "_", text.strip())
    text = re.sub(r"[^A-Za-z0-9_.-]", "", text)
    return text[:max_len] if text else "query"


def append_log_entry_csv(log_path: pathlib.Path, entry: dict):
    """Append an entry to a CSV file, create header if new file."""
    file_exists = log_path.exists()
    try:
        with log_path.open("a", encoding="utf-8", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=entry.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(entry)
    except Exception as e:
        print(f"[LOG][ERROR] Could not write to {log_path}: {e}")


def process_single_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_text(text)

def contextualize_chunks(
    chunks: list[str],
    source_file: str | None = None,
    left: int = 1,
    right: int = 1,
) -> tuple[list[str], list[dict]]:
    """
    For each 'center' chunk i, build a window:
        [i-left, ..., i-1] + [i] + [i+1, ..., i+right]
    Return:
      - enriched_texts: text used for embedding (center + neighbors)
      - metadatas: metadata about the center chunk (NOT the whole window)
    """
    n = len(chunks)
    enriched_texts: list[str] = []
    metadatas: list[dict] = []
    for i, center in enumerate(chunks):
        lo = max(0, i - left)
        hi = min(n, i + right + 1)
        window_text = "\n".join(chunks[lo:hi])

        md = {
            "source_file": source_file or "",
            "center_index": i,
            "window_left_index": lo,
            "window_right_index": hi - 1,
            # Keep a short preview of the true center for logs/citations
            "center_preview": center[:200],
            "is_contextualized": True,
        }
        enriched_texts.append(window_text)
        metadatas.append(md)
    return enriched_texts, metadatas



def build_or_reuse_index(pdf_files_list):
    sigs = file_signatures(pdf_files_list)
    if INDEX_CACHE["hybrid_retriever"] is None or signatures_changed(sigs, INDEX_CACHE["signatures"]):
        processing_log = "--- PDF Processing Log ---\n"
        per_file_docs = []
        all_images, all_tables = [], []

        if not pdf_files_list:
            INDEX_CACHE["signatures"] = None
            INDEX_CACHE["hybrid_retriever"] = None
            return processing_log + "No PDFs uploaded.\n"

        for f in pdf_files_list:
            pdf_path = f.name
            try:
                mime = filetype.guess_mime(pdf_path)
                print(f"DEBUG: Detected MIME type for {os.path.basename(pdf_path)}: {mime}")
            except Exception as e:
                mime = None
                processing_log += f"  - WARNING: Could not determine MIME type for {os.path.basename(pdf_path)}: {e}\n"

            if mime != "application/pdf":
                processing_log += f"  - SKIPPING: '{os.path.basename(pdf_path)}' is not a PDF (detected as {mime}).\n"
                continue

            processing_log += f"Processing PDF: {os.path.basename(pdf_path)}\n"
            try:
                # 1) per-page chunking with metadata
                file_docs = chunk_pdf_with_metadata(pdf_path, CHUNK_SIZE, CHUNK_OVERLAP)
                # 2) Filter very short/empty chunks
                file_docs = [d for d in file_docs if len(d.page_content.strip()) > 60]
                # 2) images/tables (unchanged)
                images_from_pdf, tables_from_pdf = extract_images_and_tables_single_pdf(pdf_path)

                per_file_docs.extend(file_docs)
                all_images.extend(images_from_pdf)
                all_tables.extend(tables_from_pdf)

                processing_log += (
                    f"  - Chunks (center) created: {len(file_docs)}\n"
                    f"  - Images extracted: {len(images_from_pdf)}\n"
                    f"  - Tables extracted: {len(tables_from_pdf)}\n"
                )
            except Exception as e:
                processing_log += f"  - ERROR processing {os.path.basename(pdf_path)}: {e}\n"
                continue

        if not per_file_docs:
            INDEX_CACHE["signatures"] = None
            INDEX_CACHE["hybrid_retriever"] = None
            return processing_log + "No text content could be extracted from any of the provided PDFs.\n"

        # 3) build contextual windows (chunk + neighbors) and index those
        ctx_docs = contextualize_documents(per_file_docs, left=1, right=1)
        processing_log += f"\n--- Combined Processing Summary ---\n"
        processing_log += f"Context-augmented vectors: {len(ctx_docs)}\n"
        processing_log += f"Images total: {len(all_images)}\n"
        processing_log += f"Tables total: {len(all_tables)}\n\n"

        # 4) Build FAISS & BM25 **from Documents** so metadata is preserved
        vectorstore = FAISS.from_documents(ctx_docs, embeddings)
        
        # --- Adaptive depth based on corpus size ---
        N = max(12, min(60, (len(ctx_docs) // 250) * 8))  # 12..60
        fetch_k = min(200, N * 6)

        # --- Dense retriever with MMR + larger fetch for diversity ---
        dense_retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": N,
                "fetch_k": fetch_k,
                "lambda_mult": 0.4,   # 0 = diversity, 1 = relevance
            },
        )

        # --- BM25 retriever (wider net) ---
        sparse_retriever = BM25Retriever.from_documents(ctx_docs)
        sparse_retriever.k = min(60, N)  # keep similar depth

        # --- Hybrid: dense + sparse ---
        hybrid = EnsembleRetriever(
            retrievers=[dense_retriever, sparse_retriever],
            weights=[0.6, 0.4],
        )

        # --- Multi-Query expansion (diversify queries) ---
        # If you defined MQR_PROMPT earlier, pass prompt=MQR_PROMPT; otherwise omit.
        mqr = MultiQueryRetriever.from_llm(
            retriever=hybrid,
            llm=llm,
            prompt=MQR_PROMPT.partial(n=str(MQR_NUM)),  
        )

        # --- Cross-encoder reranker + contextual compression ---
        ce = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
        reranker = CrossEncoderReranker(model=ce, top_n=10)  # keep the best chunks

        compression_retriever = ContextualCompressionRetriever(
            base_retriever=mqr,
            base_compressor=reranker,
        )

        # --- Cache everything you might want later ---
        INDEX_CACHE["signatures"] = sigs
        INDEX_CACHE["hybrid_retriever"] = compression_retriever  # <- use this downstream
        INDEX_CACHE["dense_retriever"] = dense_retriever
        INDEX_CACHE["sparse_retriever"] = sparse_retriever
        INDEX_CACHE["images"] = all_images
        INDEX_CACHE["tables"] = all_tables
        INDEX_CACHE["texts_count"] = len(ctx_docs)

        processing_log += "(Index built)\n"
        return processing_log

    return "(Reusing cached index — PDFs unchanged)\n"





# --- Modified Gradio interface function ---
def gradio_interface(pdf_files_list_from_gradio, query): 
    """Gradio interface function."""
    t_start = time.perf_counter()  # Start timer

    
    if not pdf_files_list_from_gradio: 
        return "Please upload at least one PDF file.", "No PDFs uploaded."

    # Pass the list of file objects to the processing function
    result, full_log, retrieved_chunks = process_pdfs_and_query(pdf_files_list_from_gradio, query)
    
    total_time = time.perf_counter() - t_start  # End timer
    full_log += f"\n=== RAG runtime ===\nTotal: {total_time:.3f} seconds\n"

    
    result = f"(Generated in {total_time:.2f}s)\n\n{result}"
    append_log_entry_csv(LOG_PATH, {
    "timestamp_utc": datetime.now(timezone.utc).isoformat(),
    "query": query,
    "answer": result,
    "processing_log": full_log,
    "retrieved_chunks": " || ".join(retrieved_chunks)  
    })

    return result, full_log


  
def main():
    """Main function to launch the Gradio interface."""
    iface = gr.Interface(
        fn=gradio_interface,
        inputs=[
            gr.File(label="Upload PDFs", file_count="multiple"), 
            gr.Textbox(label="Enter your question")
        ],
        outputs=[
            gr.Textbox(label="Answer"),
            gr.Textbox(label="Processing Log")
        ],
        title="RAG Pretest (Multiple PDFs)",
        description="Langchain+Chatgpt4omini Pretest UI for WELL Certification with multi-PDF support."
    )
    
    iface.launch()

if __name__ == "__main__":
    main()
