#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
RAG Evaluation (Relevance & Accuracy) with RAGAS
- Supports black-box RAG via RubricsScore.
- Supports retrieval-aware scoring via ResponseRelevancy and answer_correctness.
- Outputs per-query scores, aggregated stats, and bootstrap CIs.
"""
import asyncio
import atexit
import json
import math
import os
import random
import re
from pathlib import Path
from typing import Any, List, Coroutine, Dict, Tuple
import numpy as np
import pandas as pd
from datasets import Dataset
from dotenv import find_dotenv, load_dotenv
from tqdm import tqdm
from langchain_openai import ChatOpenAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from ragas import evaluate
from ragas.dataset_schema import SingleTurnSample
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper
from ragas.metrics import ResponseRelevancy, RubricsScore, answer_correctness

try:
    from unidecode import unidecode
    USE_UNIDECODE = True
except ImportError:
    USE_UNIDECODE = False

load_dotenv(find_dotenv())
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# --- Configuration ---
INPUT_CSV = Path("testset.csv")
OUTPUT_RESULTS_CSV = Path("rag_eval_results.csv")
OUTPUT_SUMMARY_CSV = Path("rag_eval_summary.csv")

# Rubric templates can be edited here or loaded from a file
RUBRIC_RELEVANCE = {
    "score1_description": "The response is entirely off-topic or irrelevant to the user query.",
    "score2_description": "Mostly off-topic; only tangentially related to the query.",
    "score3_description": "Addresses the general topic but not the specific question or includes substantial irrelevant information.",
    "score4_description": "Mostly focused on the query with minor digressions or unnecessary details.",
    "score5_description": "Fully relevant and focused on the user query.",
}
RUBRIC_ACCURACY = {
    "score1_description": "Completely incorrect, contradicts the reference, or fabricates facts.",
    "score2_description": "Partially accurate but with major errors or omissions that change meaning.",
    "score3_description": "Mostly accurate but missing key details or containing minor errors.",
    "score4_description": "Accurate with only minor omissions or slight inaccuracies.",
    "score5_description": "Fully accurate, precise, and consistent with the reference.",
}

# --- Text and Data Helpers ---

def normalize_text(s: Any) -> str:
    """Converts a value to a normalized string for comparison."""
    if s is None or (isinstance(s, float) and math.isnan(s)):
        return ""

    s = str(s).strip()
    if USE_UNIDECODE:
        s = unidecode(s)
    s = s.lower()
    s = re.sub(r"\s+", " ", s)
    return s

def _sanitize_retrieved_docs(docs_str: str) -> str:
    """Ensure the retrieved_docs string is a valid JSON list string."""
    docs_str = (docs_str or "").strip()
    if not docs_str:
        return "[]"
    # If it looks like a JSON array or object, trust it.
    if (docs_str.startswith("[") and docs_str.endswith("]")) or \
            (docs_str.startswith("{") and docs_str.endswith("}")):
        return docs_str
    # Otherwise, treat as a single document string and wrap in a JSON list.
    return json.dumps([docs_str], ensure_ascii=False)

def load_and_prepare_dataset(path: Path) -> pd.DataFrame:
    """Loads the dataset from a CSV and prepares the necessary columns."""
    df = pd.read_csv(
        path,
        dtype=str,
        keep_default_na=False,
        na_filter=False,
        encoding="utf-8-sig"
    )

    required = {"query", "grounded_answer", "rag_output"}
    if not required.issubset(df.columns):
        raise ValueError(f"CSV is missing required columns: {required - set(df.columns)}")

    for col in required:
        df[col] = df[col].astype(str).fillna("")

    if "retrieved_docs" not in df.columns:
        df["retrieved_docs"] = "[]"
    else:
        df["retrieved_docs"] = df["retrieved_docs"].astype(str).fillna("").apply(_sanitize_retrieved_docs)

    # Create normalized versions for potential analysis
    df["query_norm"] = df["query"].apply(normalize_text)
    df["grounded_norm"] = df["grounded_answer"].apply(normalize_text)
    df["rag_norm"] = df["rag_output"].apply(normalize_text)

    return df

def to_hf_dataset_for_correctness(df: pd.DataFrame) -> Dataset:
    """Converts a DataFrame to a Hugging Face Dataset for the correctness metric."""
    return Dataset.from_dict({
        "question": df["query"].tolist(),
        "answer": df["rag_output"].tolist(),
        "ground_truth": df["grounded_answer"].tolist(),
    })

# --- Evaluator Setup ---

def setup_openai_evaluator(model_name: str = "gpt-4o") -> LangchainLLMWrapper:
    """Initializes the OpenAI LLM for evaluation."""
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("Please set OPENAI_API_KEY in your .env file")
    try:
        chat = ChatOpenAI(model=model_name, temperature=0)
        return LangchainLLMWrapper(chat)
    except ImportError:
        raise RuntimeError("Please `pip install langchain_openai` to use OpenAI evaluators.")

def setup_google_embeddings() -> LangchainEmbeddingsWrapper:
    """Initializes Google Generative AI embeddings for evaluation."""
    if not os.getenv("GOOGLE_API_KEY"):
        raise ValueError("Please set GOOGLE_API_KEY in your .env file")
    try:
        ge = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        return LangchainEmbeddingsWrapper(ge)
    except ImportError:
        raise RuntimeError("Please `pip install langchain-google-genai` to use Google embeddings.")

def initialize_evaluators() -> Tuple[LangchainLLMWrapper, LangchainEmbeddingsWrapper]:
    """Sets up and returns the LLM and embedding models for RAGAS."""
    print("Initializing evaluator LLM and embeddingsâ€¦")
    evaluator_llm = setup_openai_evaluator()
    evaluator_embeddings = setup_google_embeddings()
    return evaluator_llm, evaluator_embeddings

# --- Async and Scoring Helpers ---

def run_sync(coro: Coroutine) -> Any:
    """Runs a coroutine synchronously, handling different RAGAS versions."""
    if hasattr(coro, "__await__"):
        return asyncio.get_event_loop().run_until_complete(coro)
    return coro

def bootstrap_ci(values: List[float], n_boot: int = 2000, alpha: float = 0.05) -> Tuple[float, float]:
    """Calculates the bootstrap confidence interval for a list of values."""
    if not values:
        return (math.nan, math.nan)
    rng = np.random.default_rng(SEED)
    values = np.array(values, dtype=float)
    means = [rng.choice(values, size=len(values), replace=True).mean() for _ in range(n_boot)]
    return np.percentile(means, [100 * (alpha / 2), 100 * (1 - alpha / 2)])

# --- Core Evaluation Logic ---

def calculate_scores(df: pd.DataFrame, llm: LangchainLLMWrapper, embeddings: LangchainEmbeddingsWrapper) -> pd.DataFrame:
    """Calculates all RAGAS scores for the given dataset."""

    # --- Batch-calculate answer_correctness (0-1) ---
    print("Scoring 'answer_correctness' across the dataset...")
    hf_dataset = to_hf_dataset_for_correctness(df)
    correctness_results = evaluate(hf_dataset, metrics=[answer_correctness], llm=llm, embeddings=embeddings)
    correctness_list = correctness_results.to_pandas()["answer_correctness"].tolist()

    # --- Initialize scorers for per-row evaluation ---
    relevance_rubric_scorer = RubricsScore(rubrics=RUBRIC_RELEVANCE, llm=llm)
    accuracy_rubric_scorer = RubricsScore(rubrics=RUBRIC_ACCURACY, llm=llm)
    response_relevancy_scorer = ResponseRelevancy(llm=llm, embeddings=embeddings)

    # --- Score each row for remaining metrics ---
    print("Scoring relevance and accuracy per query...")
    per_row_records = []
    for i, row in tqdm(df.iterrows(), total=len(df)):
        query, reference, response = row["query"], row["grounded_answer"], row["rag_output"]

        # Rubrics-based scores (1-5)
        rel_rubric = run_sync(relevance_rubric_scorer.single_turn_ascore(SingleTurnSample(user_input=query, response=response)))
        acc_rubric = run_sync(accuracy_rubric_scorer.single_turn_ascore(SingleTurnSample(response=response, reference=reference)))

        # Response Relevancy (0-1)
        resp_relevancy = run_sync(response_relevancy_scorer.single_turn_ascore(SingleTurnSample(user_input=query, response=response)))

        per_row_records.append({
            "idx": i,
            "query": query,
            "gold": reference,
            "response": response,
            "relevance_rubric_1to5": float(rel_rubric),
            "accuracy_rubric_1to5": float(acc_rubric),
            "answer_correctness_0to1": float(correctness_list[i]),
            "response_relevancy_0to1": float(resp_relevancy),
        })

    return pd.DataFrame(per_row_records)

def generate_summary(df_scores: pd.DataFrame) -> pd.DataFrame:
    """Generates a summary DataFrame with aggregated statistics."""

    def agg(col: str) -> Dict[str, Any]:
        vals = df_scores[col].dropna().tolist()
        if not vals:
            return {"n": 0, "mean": math.nan, "std": math.nan, "ci95_low": math.nan, "ci95_high": math.nan}

        low, high = bootstrap_ci(vals)
        return {
            "n": len(vals),
            "mean": float(np.mean(vals)),
            "std": float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0,
            "ci95_low": float(low),
            "ci95_high": float(high),
        }

    summary_rows = []
    for metric in ["relevance_rubric_1to5", "accuracy_rubric_1to5", "answer_correctness_0to1", "response_relevancy_0to1"]:
        stats = agg(metric)
        stats["metric"] = metric
        summary_rows.append(stats)

    return pd.DataFrame(summary_rows)

# --- Main Execution ---

def main():
    """Main function to run the RAG evaluation pipeline."""
    df = load_and_prepare_dataset(INPUT_CSV)
    evaluator_llm, evaluator_embeddings = initialize_evaluators()

    df_scores = calculate_scores(df, evaluator_llm, evaluator_embeddings)
    df_scores.to_csv(OUTPUT_RESULTS_CSV, index=False)
    print(f"\nSaved per-query results to {OUTPUT_RESULTS_CSV}")

    df_summary = generate_summary(df_scores)
    df_summary.to_csv(OUTPUT_SUMMARY_CSV, index=False)
    print(f"Saved summary to {OUTPUT_SUMMARY_CSV}")

    print("\n=== Aggregated Results ===")
    print(df_summary.to_string(index=False))

def _shutdown_grpc_aio():
    """Gracefully shutdown gRPC asyncio channels on exit."""
    try:
        from grpc import aio as grpc_aio
        grpc_aio.shutdown_grpc_aio()
    except Exception:
        pass

if __name__ == "__main__":
    atexit.register(_shutdown_grpc_aio)
    main()